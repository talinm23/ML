{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/YqTOFHXyah+U40+HW2b5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/talinm23/ML/blob/main/step2_letssee.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5WLY21lYjh6s"
      },
      "outputs": [],
      "source": [
        "# In order to run these codes (parsing pdfs step 1 and 2), you can run them in a Google Colab notebook.\n",
        "# To run fast, you should get connected to a GPU in the Colab environment.\n",
        "# In the Colab secrets, create and copy your HUGGING_FACE_TOKEN and MONGO_URI (MongoDB URI) to the Secrets section."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear all variables\n",
        "%reset -f\n",
        "#Check for Persistent Extensions or Cache Files\n",
        "import os\n",
        "import shutil\n",
        "cache_dir = os.path.expanduser('~/.cache')\n",
        "shutil.rmtree(cache_dir, ignore_errors=True)"
      ],
      "metadata": {
        "id": "hD423DIa63_s"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKmE3XSo7gWC",
        "outputId": "a5a80b5f-9e73-404b-b4b8-088834fc97de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports and granting access to the google drive to extrace the saved embedding vector information from the step 1 notebook.\n",
        "import pandas as pd\n",
        "from ast import literal_eval\n",
        "\n",
        "# Get connected to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "li_-2hvi64CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the saved file from step 1.\n",
        "file_path_to_save = '/content/drive/My Drive/Colab_Notebooks/csv_saved/'\n",
        "dataset = pd.read_csv(file_path_to_save+'dataset_embedded.csv')"
      ],
      "metadata": {
        "id": "xfbs3beg64Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply literal_eval on the \"embeddings\" column to extract the list inside the string in each row and save it into a new column.\n",
        "dataset['embedding'] = dataset['embedding_'].apply(literal_eval)"
      ],
      "metadata": {
        "id": "D5qQ4KQz7EAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymongo"
      ],
      "metadata": {
        "id": "ud-ThpI-7EC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connecting to MongoDB\n",
        "import pymongo\n",
        "from google.colab import userdata\n",
        "\n",
        "def get_mongo_client(mongo_uri):\n",
        "  \"\"\"Establish connection to the MongoDB.\"\"\"\n",
        "  try:\n",
        "    client = pymongo.MongoClient(mongo_uri, appname=\"devrel.content.python\"\n",
        "    #,ssl=True,tlsAllowInvalidCertificates=True\n",
        "    ,connectTimeoutMS=40000\n",
        "    ,socketTimeoutMS=40000)\n",
        "    print(\"Connection to MongoDB successful\")\n",
        "    return client\n",
        "  except pymongo.errors.ConnectionFailure as e:\n",
        "    print(f\"Connection failed: {e}\")\n",
        "    return None\n",
        "\n",
        "mongo_uri = userdata.get('MONGO_URI')\n",
        "if not mongo_uri:\n",
        "  print(\"MONGO_URI not set in environment variables\")\n",
        "\n",
        "mongo_client = get_mongo_client(mongo_uri)\n",
        "\n",
        "# Ingest data into MongoDB (into the created collection)\n",
        "db = mongo_client['coldwell']\n",
        "collection = db['coldwell_collection']\n",
        "print('db:',db)\n",
        "print('collection:',collection)"
      ],
      "metadata": {
        "id": "nGbCclBz7EFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete any existing records in the collection just in case. So we start with an empty collection.\n",
        "collection.delete_many({})"
      ],
      "metadata": {
        "id": "QKuz98Jc7EIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dataset into a list of dictionary, where each data row is a new record.\n",
        "# Then insert that documents variable in batch into the collection.\n",
        "documents = dataset.to_dict(\"records\")\n",
        "collection.insert_many(documents)\n",
        "print(\"Data ingestion into MongoDB completed\")"
      ],
      "metadata": {
        "id": "8YTRLHOR7ELB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# https://huggingface.co/thenlper/gte-large\n",
        "embedding_model = SentenceTransformer(\"thenlper/gte-large\")\n",
        "\n",
        "\n",
        "def get_embedding(text: str) -> list[float]:\n",
        "    if not text.strip():\n",
        "        print(\"Attempted to get embedding for empty text.\")\n",
        "        return []\n",
        "\n",
        "    embedding = embedding_model.encode(text)\n",
        "\n",
        "    return embedding.tolist()"
      ],
      "metadata": {
        "id": "ErXsYy8X64Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we perform a vector search in the MongoDB collection based on the\n",
        "# user query. We pass in the user's query string.\n",
        "# And it searches the MongoDB collection and returns a list of matchig documents.\n",
        "\n",
        "# Generate embedding for the user query\n",
        "query = \"What is the name of the author? what are the natural remedies according to the book? list the natural remedies from the book.\"\n",
        "\n",
        "query_embedding = get_embedding(query)\n",
        "\n",
        "# Define the vector search pipeline\n",
        "vector_search_stage = {\n",
        "    \"$vectorSearch\": {\n",
        "        \"index\": \"vector_index\",\n",
        "        \"queryVector\": query_embedding,\n",
        "        \"path\": \"embedding\",\n",
        "        \"numCandidates\": 1000 ,  # Number of candidate matches to consider\n",
        "        \"limit\": 50 # Return top n matches\n",
        "    }\n",
        "}\n",
        "\n",
        "project_stage = {\n",
        "    \"$project\": {\n",
        "        \"_id\": 1,  # Can exclude the _id field\n",
        "        \"sentences\": 1, # Include the sentences fields so that we see the actual sentences.\n",
        "        \"score\": {\n",
        "            \"$meta\": \"vectorSearchScore\"  # Include the search score\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "pipeline = [vector_search_stage, project_stage]\n",
        "results = collection.aggregate(pipeline)\n"
      ],
      "metadata": {
        "id": "PCzguhD07RSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the search\n",
        "results = collection.aggregate(pipeline)\n",
        "\n",
        "# Define Get knowledge by the list of results returned from the vector search.\n",
        "get_knowledge = list(results)\n",
        "\n",
        "# Add all the qualifying sentences from the vector search.\n",
        "search_result = \"\"\n",
        "for result in get_knowledge:\n",
        "    search_result += f\"result: {result.get('sentences')}\\n\""
      ],
      "metadata": {
        "id": "DXranZjr7RVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "source_information = search_result\n",
        "combined_information = (\n",
        "    f\"Query: {query}\\nContinue to answer the query by using the Search Results:\\n{source_information}.\"\n",
        ")"
      ],
      "metadata": {
        "id": "DAhw8QG17RX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "from huggingface_hub import login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "QoIpy3G-7Ra7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "# CPU Enabled uncomment below üëáüèΩ\n",
        "# model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\")\n",
        "# GPU Enabled use below üëáüèΩ\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "UuN5MACX7YF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Moving tensors to GPU\n",
        "input_ids = tokenizer(combined_information, return_tensors=\"pt\").to(\"cuda\")\n",
        "response = model.generate(**input_ids, max_new_tokens=1000)\n",
        "print(tokenizer.decode(response[0]))"
      ],
      "metadata": {
        "id": "E_0aENak7YIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5177hnVB7YMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PX6Zw1YbP-W5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}