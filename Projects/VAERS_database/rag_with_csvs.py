# -*- coding: utf-8 -*-
"""RAG_WT_CSV_VAERS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qRtEfPJPdcRIu_PQsuw7EQVcIoohuUg8
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import time
from collections import Counter
from tqdm.auto import tqdm
import openai
import os
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', None)
# Install required packages
#!pip install faiss-cpu langchain langchain-community langchain-openai pandas python-dotenv
from langchain_community.document_loaders.csv_loader import CSVLoader
from pathlib import Path
from langchain_openai import ChatOpenAI,OpenAIEmbeddings
from dotenv import load_dotenv
import faiss
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain

# Load environment variables from a .env file
load_dotenv()

api_key1="enter your OPENAI API key here"
client = openai.OpenAI(api_key=api_key1)
openai.api_key = api_key1

# Set the OpenAI API key environment variable
#os.environ["OPENAI_API_KEY"] = os.getenv('OPENAI_API_KEY')

#llm = ChatOpenAI(model="gpt-3.5-turbo-0125",openai_api_key=api_key1)
llm = ChatOpenAI(model="gpt-4o", temperature=0, max_completion_tokens=500, openai_api_key=api_key1)
csv_file = 'VAERS_DATA.csv'

'''
# Run once to save the chunks of csv files.
# Create output directory if it doesn't exist
output_dir = "csv_chunk_files"
os.makedirs(output_dir, exist_ok=True)
# Load and split the large CSV
chunk_size = 400  # Number of rows per chunk
for i, chunk in enumerate(pd.read_csv(csv_file, chunksize=chunk_size)):
    chunk_file = os.path.join(output_dir, f"chunk_{i}.csv")
    chunk.to_csv(chunk_file, index=False)
    print(f"Saved {chunk_file}")
'''

embeddings = OpenAIEmbeddings(openai_api_key=api_key1)
index = faiss.IndexFlatL2(len(embeddings.embed_query(" ")))
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={})

'''
# Run once when trying to save the chunks.
# Load each chunked CSV and add to vector store
chunk_dir = "csv_chunk_files"
csv_files = [
    os.path.join(chunk_dir, filename)
    for filename in os.listdir(chunk_dir)
    if filename.endswith(".csv")]
'''
'''
# Uncomment to get CSV loader running to then save it into the vector_stor.
start_time = time.time()
for csv_file in csv_files:
    loader = CSVLoader(file_path=csv_file)
    docs = loader.load() #loader.load_and_split()
    vector_store.add_documents(documents=docs)
    time.sleep(9)
end_time = time.time()
elapsed_time = end_time - start_time
print(f"Elapsed time (docs = loader.load_and_split()): {elapsed_time:.4f} seconds, for {i} chunks")
# Save to disk
vector_store.save_local("faiss_index")
'''

#to access:
vector_store = FAISS.load_local(
    "faiss_index",
    embeddings=embeddings,
    allow_dangerous_deserialization=True)

#retriever = vector_store.as_retriever()
retriever = vector_store.as_retriever(search_kwargs={"k": 123, "score_threshold": 0.5})

# Set up system prompt
system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}")

prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}"),])

# Create the question-answer chain
question_answer_chain = create_stuff_documents_chain(llm, prompt)
rag_chain = create_retrieval_chain(retriever, question_answer_chain)

answer= rag_chain.invoke({"input": "Which vaccine has the most symptoms?"
                                   "Which vaccine causes the most deaths?"
                                   "What duration is the most frequent that the death occurred?"
                                   "Share some interesting statistics."
                                   "Can you conclude that the deaths were related to the vaccine?"
                                   "Explain how many records you are looking at."})

print(answer['answer'])
print('Done')



'''
#-----------------------------------------------------------------------------------------------------------------
#Extra: 
# If you want to summarize the CSV before passing to the LLM:
# Use a map-reduce summarization over your CSV docs/chunks, then feed the summaries to your RAG prompt.
# Here's how to do it for this code, but since it is computationally expensive, we did not do it.
import os
from langchain_community.document_loaders import CSVLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains.summarize import load_summarize_chain
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.schema import Document

# --- setup ---
llm = ChatOpenAI(model="gpt-4o", temperature=0, max_completion_tokens=300)
sum_chain = load_summarize_chain(llm, chain_type="map_reduce")
text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=120)

# --- iterate CSV chunks ---
chunk_dir = "csv_chunk_files"
csv_files = [
    os.path.join(chunk_dir, f) for f in os.listdir(chunk_dir) if f.endswith(".csv")
]

for csv_file in csv_files:
    loader = CSVLoader(file_path=csv_file)
    # If you *must* use load_and_split, keep the next line and remove the .load() path:
    chunks = loader.load_and_split(text_splitter=text_splitter)

    # Map-reduce summarize this fileâ€™s chunks to a single concise string
    summary_text = sum_chain.run(chunks)

    # Store the summary (one doc per CSV) instead of the raw rows
    summary_doc = [Document(page_content=summary_text,
                            metadata={"source": os.path.basename(csv_file),
                                      "type": "summary"})]
    vector_store.add_documents(summary_doc)
#-----------------------------------------------------------------------------------------------------------------
'''